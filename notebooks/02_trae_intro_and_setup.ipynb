{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Trae AI and Setup\n",
    "\n",
    "This notebook introduces Trae AI and demonstrates how to integrate it with your LLM workflows.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Trae AI architecture and capabilities\n",
    "- Set up Trae AI environment\n",
    "- Connect to Trae AI services\n",
    "- Explore Trae AI model management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Trae AI imports (placeholder - replace with actual imports)\n",
    "# from trae_ai import TraeClient, TraeModel, TraeTrainer\n",
    "\n",
    "print(\"Environment setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Trae AI?\n",
    "\n",
    "Trae AI is a powerful platform for LLM development that provides:\n",
    "\n",
    "- **Advanced Training**: Efficient fine-tuning with state-of-the-art techniques\n",
    "- **Model Management**: Version control and deployment for ML models\n",
    "- **Scalable Infrastructure**: Cloud-native solutions for LLM workloads\n",
    "- **Integration Tools**: Seamless integration with popular ML frameworks\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Distributed Training**: Scale training across multiple GPUs and nodes\n",
    "2. **Memory Optimization**: Advanced techniques like gradient checkpointing\n",
    "3. **Model Serving**: High-performance inference endpoints\n",
    "4. **Monitoring**: Real-time training and inference monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Trae AI\n",
    "\n",
    "Let's configure the Trae AI environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Trae AI\n",
    "TRAE_CONFIG = {\n",
    "    'api_endpoint': os.getenv('TRAE_API_ENDPOINT', 'https://api.trae.ai'),\n",
    "    'api_key': os.getenv('TRAE_API_KEY', 'your-api-key-here'),\n",
    "    'project_name': 'llm-lab-demo',\n",
    "    'workspace': 'default'\n",
    "}\n",
    "\n",
    "# Check if API key is set\n",
    "if TRAE_CONFIG['api_key'] == 'your-api-key-here':\n",
    "    print(\"‚ö†Ô∏è  Please set your TRAE_API_KEY environment variable\")\n",
    "    print(\"   You can get an API key from: https://trae.ai/dashboard\")\n",
    "else:\n",
    "    print(\"‚úÖ Trae AI API key configured\")\n",
    "\n",
    "print(f\"Configuration: {TRAE_CONFIG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for Trae AI client initialization\n",
    "# This would be the actual Trae AI client setup\n",
    "\n",
    "class MockTraeClient:\n",
    "    \"\"\"Mock Trae AI client for demonstration purposes\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.connected = False\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Simulate connection to Trae AI\"\"\"\n",
    "        print(\"üîó Connecting to Trae AI...\")\n",
    "        # Simulate API call\n",
    "        self.connected = True\n",
    "        print(\"‚úÖ Connected to Trae AI successfully!\")\n",
    "        return True\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        if not self.connected:\n",
    "            raise Exception(\"Not connected to Trae AI\")\n",
    "        \n",
    "        # Mock model list\n",
    "        return [\n",
    "            {'name': 'trae-llama-7b', 'type': 'causal-lm', 'size': '7B'},\n",
    "            {'name': 'trae-mistral-7b', 'type': 'causal-lm', 'size': '7B'},\n",
    "            {'name': 'trae-codellama-13b', 'type': 'code-lm', 'size': '13B'}\n",
    "        ]\n",
    "    \n",
    "    def get_model_info(self, model_name):\n",
    "        \"\"\"Get detailed model information\"\"\"\n",
    "        models_info = {\n",
    "            'trae-llama-7b': {\n",
    "                'name': 'trae-llama-7b',\n",
    "                'description': 'Optimized Llama 7B model with Trae enhancements',\n",
    "                'parameters': '7B',\n",
    "                'context_length': 4096,\n",
    "                'training_data': 'Curated high-quality dataset',\n",
    "                'performance': {'throughput': '150 tokens/s', 'latency': '20ms'}\n",
    "            }\n",
    "        }\n",
    "        return models_info.get(model_name, {'error': 'Model not found'})\n",
    "\n",
    "# Initialize Trae client\n",
    "trae_client = MockTraeClient(TRAE_CONFIG)\n",
    "trae_client.connect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring Trae AI Models\n",
    "\n",
    "Let's explore the available models in Trae AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "models = trae_client.list_models()\n",
    "\n",
    "print(\"Available Trae AI Models:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for model in models:\n",
    "    print(f\"üì¶ {model['name']}\")\n",
    "    print(f\"   Type: {model['type']}\")\n",
    "    print(f\"   Size: {model['size']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about a specific model\n",
    "model_name = 'trae-llama-7b'\n",
    "model_info = trae_client.get_model_info(model_name)\n",
    "\n",
    "print(f\"Model Information: {model_name}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, value in model_info.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"{key.title()}:\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            print(f\"  {sub_key.title()}: {sub_value}\")\n",
    "    else:\n",
    "        print(f\"{key.title()}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trae AI Integration Patterns\n",
    "\n",
    "Learn common patterns for integrating Trae AI with your workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Trae AI model wrapper\n",
    "class TraeModelWrapper:\n",
    "    \"\"\"Wrapper for Trae AI models with HuggingFace compatibility\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, client):\n",
    "        self.model_name = model_name\n",
    "        self.client = client\n",
    "        self.model_info = client.get_model_info(model_name)\n",
    "        \n",
    "        # For demo, we'll use a HuggingFace model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        self.model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def generate(self, prompt, **kwargs):\n",
    "        \"\"\"Generate text using Trae AI optimizations\"\"\"\n",
    "        # In real implementation, this would use Trae AI's optimized inference\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "        \n",
    "        # Apply Trae AI optimizations (simulated)\n",
    "        default_params = {\n",
    "            'max_length': inputs.shape[1] + 50,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.9,\n",
    "            'pad_token_id': self.tokenizer.eos_token_id\n",
    "        }\n",
    "        default_params.update(kwargs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(inputs, **default_params)\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text[len(prompt):].strip()\n",
    "    \n",
    "    def get_performance_metrics(self):\n",
    "        \"\"\"Get model performance metrics\"\"\"\n",
    "        return self.model_info.get('performance', {})\n",
    "\n",
    "# Initialize Trae model\n",
    "trae_model = TraeModelWrapper('trae-llama-7b', trae_client)\n",
    "print(f\"‚úÖ Trae model '{trae_model.model_name}' loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Trae AI model generation\n",
    "prompts = [\n",
    "    \"The benefits of using Trae AI for LLM development include\",\n",
    "    \"In the future, artificial intelligence will\",\n",
    "    \"The most important aspect of machine learning is\"\n",
    "]\n",
    "\n",
    "print(\"Trae AI Model Generation Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    response = trae_model.generate(prompt, max_length=100)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Monitoring\n",
    "\n",
    "Monitor model performance and resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model, prompts, num_runs=3):\n",
    "    \"\"\"Benchmark model performance\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        times = []\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            response = model.generate(prompt, max_length=50)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        \n",
    "        results.append({\n",
    "            'prompt': prompt[:50] + '...' if len(prompt) > 50 else prompt,\n",
    "            'avg_time': avg_time,\n",
    "            'std_time': std_time,\n",
    "            'tokens_per_second': len(response.split()) / avg_time\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Benchmark the model\n",
    "test_prompts = [\n",
    "    \"Write a short story about\",\n",
    "    \"Explain the concept of\",\n",
    "    \"List the advantages of\"\n",
    "]\n",
    "\n",
    "print(\"Benchmarking Trae AI model...\")\n",
    "benchmark_results = benchmark_model(trae_model, test_prompts)\n",
    "\n",
    "print(\"\nBenchmark Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for result in benchmark_results:\n",
    "    print(f\"Prompt: {result['prompt']}\")\n",
    "    print(f\"Average time: {result['avg_time']:.3f}s (¬±{result['std_time']:.3f}s)\")\n",
    "    print(f\"Tokens/second: {result['tokens_per_second']:.1f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration Management\n",
    "\n",
    "Learn how to manage configurations for different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration management example\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Sample configuration\n",
    "config = {\n",
    "    'trae': {\n",
    "        'api_endpoint': 'https://api.trae.ai',\n",
    "        'model_name': 'trae-llama-7b',\n",
    "        'max_tokens': 512,\n",
    "        'temperature': 0.7\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 8,\n",
    "        'learning_rate': 2e-5,\n",
    "        'num_epochs': 3,\n",
    "        'warmup_steps': 100\n",
    "    },\n",
    "    'data': {\n",
    "        'train_file': 'data/train.jsonl',\n",
    "        'val_file': 'data/val.jsonl',\n",
    "        'max_length': 512\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = Path('../trae_llm/config.yaml')\n",
    "config_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úÖ Configuration saved to {config_path}\")\n",
    "\n",
    "# Load and display configuration\n",
    "with open(config_path, 'r') as f:\n",
    "    loaded_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"\nLoaded Configuration:\")\n",
    "print(yaml.dump(loaded_config, default_flow_style=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with Existing Workflows\n",
    "\n",
    "See how Trae AI integrates with popular ML tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Integration with Weights & Biases\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_available = True\n",
    "except ImportError:\n",
    "    wandb_available = False\n",
    "\n",
    "if wandb_available:\n",
    "    print(\"‚úÖ Weights & Biases integration available\")\n",
    "    \n",
    "    # Mock W&B integration\n",
    "    def log_trae_metrics(model_name, metrics):\n",
    "        \"\"\"Log Trae AI metrics to W&B\"\"\"\n",
    "        # In real implementation:\n",
    "        # wandb.log({\n",
    "        #     f'trae/{model_name}/throughput': metrics.get('throughput'),\n",
    "        #     f'trae/{model_name}/latency': metrics.get('latency'),\n",
    "        #     f'trae/{model_name}/memory_usage': metrics.get('memory_usage')\n",
    "        # })\n",
    "        print(f\"üìä Logged metrics for {model_name}: {metrics}\")\n",
    "    \n",
    "    # Example usage\n",
    "    performance_metrics = trae_model.get_performance_metrics()\n",
    "    log_trae_metrics(trae_model.model_name, performance_metrics)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Weights & Biases not installed. Install with: pip install wandb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Great! You've learned the basics of Trae AI integration. Continue with:\n",
    "\n",
    "- **03_fine_tuning_with_trae.ipynb**: Fine-tune models using Trae AI\n",
    "- **04_rag_with_trae.ipynb**: Build RAG systems with Trae\n",
    "- **05_evaluation_and_visualization.ipynb**: Evaluate and visualize results\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Trae AI** provides advanced LLM capabilities with optimized performance\n",
    "2. **Integration** is seamless with existing HuggingFace workflows\n",
    "3. **Configuration management** enables reproducible experiments\n",
    "4. **Performance monitoring** helps optimize model deployment\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Try these exercises:\n",
    "\n",
    "1. Set up your own Trae AI API key and test the connection\n",
    "2. Experiment with different model configurations\n",
    "3. Create a custom wrapper for your specific use case\n",
    "4. Integrate Trae AI with your preferred monitoring tool\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}