{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Inference Basics\n",
    "\n",
    "This notebook introduces the fundamentals of using HuggingFace Transformers for model inference.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load pre-trained models from HuggingFace Hub\n",
    "- Perform text generation with different models\n",
    "- Understand tokenization and model outputs\n",
    "- Explore different model architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Text Generation with Pipeline\n",
    "\n",
    "The easiest way to get started with HuggingFace is using pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=100)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=2)\n",
    "\n",
    "for i, text in enumerate(result):\n",
    "    print(f\"Generation {i+1}: {text['generated_text']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual Model Loading and Tokenization\n",
    "\n",
    "For more control, we can load models and tokenizers manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization example\n",
    "text = \"Hello, how are you today?\"\n",
    "\n",
    "# Encode text\n",
    "encoded = tokenizer.encode(text, return_tensors='pt')\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Encoded tokens: {encoded}\")\n",
    "print(f\"Token IDs: {encoded[0].tolist()}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(encoded[0])\n",
    "print(f\"Decoded text: {decoded}\")\n",
    "\n",
    "# Show individual tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Individual tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Chat Example\n",
    "\n",
    "Let's create a simple chatbot using DialoGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(model, tokenizer, user_input, chat_history_ids=None):\n",
    "    \"\"\"Generate a response using the model\"\"\"\n",
    "    # Encode user input\n",
    "    new_user_input_ids = tokenizer.encode(\n",
    "        user_input + tokenizer.eos_token, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Append to chat history\n",
    "    if chat_history_ids is not None:\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "    else:\n",
    "        bot_input_ids = new_user_input_ids\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        chat_history_ids = model.generate(\n",
    "            bot_input_ids,\n",
    "            max_length=1000,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(\n",
    "        chat_history_ids[:, bot_input_ids.shape[-1]:][0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response, chat_history_ids\n",
    "\n",
    "# Example conversation\n",
    "chat_history = None\n",
    "\n",
    "user_inputs = [\n",
    "    \"Hello! How are you?\",\n",
    "    \"What's your favorite programming language?\",\n",
    "    \"Tell me a joke\"\n",
    "]\n",
    "\n",
    "for user_input in user_inputs:\n",
    "    print(f\"User: {user_input}\")\n",
    "    response, chat_history = chat_with_model(model, tokenizer, user_input, chat_history)\n",
    "    print(f\"Bot: {response}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring Different Models\n",
    "\n",
    "Let's try different types of models for various tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "texts = [\n",
    "    \"I love this new AI technology!\",\n",
    "    \"This is terrible and frustrating.\",\n",
    "    \"The weather is okay today.\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "for text in texts:\n",
    "    result = classifier(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result[0]['label']} (confidence: {result[0]['score']:.3f})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question answering\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"\n",
    "The Transformer architecture was introduced in the paper \"Attention Is All You Need\" \n",
    "by Vaswani et al. in 2017. It revolutionized natural language processing by using \n",
    "self-attention mechanisms instead of recurrent or convolutional layers.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was the Transformer architecture introduced?\",\n",
    "    \"What did the Transformer architecture use instead of recurrent layers?\",\n",
    "    \"Who introduced the Transformer architecture?\"\n",
    "]\n",
    "\n",
    "print(\"Question Answering Results:\")\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']} (confidence: {result['score']:.3f})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Information and Exploration\n",
    "\n",
    "Let's explore model architectures and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore model configuration\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"Model type: {model.config.model_type}\")\n",
    "print(f\"Hidden size: {model.config.n_embd}\")\n",
    "print(f\"Number of layers: {model.config.n_layer}\")\n",
    "print(f\"Number of attention heads: {model.config.n_head}\")\n",
    "print(f\"Vocabulary size: {model.config.vocab_size}\")\n",
    "print(f\"Maximum position embeddings: {model.config.n_positions}\")\n",
    "\n",
    "# Model size calculation\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024**2:.1f} MB (float32)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation Parameters\n",
    "\n",
    "Experiment with different generation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generation_strategies(model, tokenizer, prompt):\n",
    "    \"\"\"Compare different text generation strategies\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    strategies = {\n",
    "        'Greedy': {'do_sample': False},\n",
    "        'Beam Search': {'num_beams': 5, 'do_sample': False},\n",
    "        'Top-k Sampling': {'do_sample': True, 'top_k': 50},\n",
    "        'Top-p Sampling': {'do_sample': True, 'top_p': 0.9},\n",
    "        'Temperature': {'do_sample': True, 'temperature': 0.7}\n",
    "    }\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for strategy_name, params in strategies.items():\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_length=input_ids.shape[1] + 30,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                **params\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        new_text = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"{strategy_name}: {new_text}\")\n",
    "        print()\n",
    "\n",
    "# Test with a creative prompt\n",
    "creative_prompt = \"Once upon a time in a magical forest\"\n",
    "compare_generation_strategies(model, tokenizer, creative_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations! You've learned the basics of HuggingFace inference. Next, explore:\n",
    "\n",
    "- **02_trae_intro_and_setup.ipynb**: Introduction to Trae AI integration\n",
    "- **03_fine_tuning_with_trae.ipynb**: Fine-tuning models with Trae\n",
    "- **04_rag_with_trae.ipynb**: Building RAG systems\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Try the following exercises to reinforce your learning:\n",
    "\n",
    "1. Load a different model (e.g., 'distilgpt2', 'microsoft/DialoGPT-small')\n",
    "2. Experiment with different generation parameters\n",
    "3. Create a simple text summarization pipeline\n",
    "4. Build a multi-turn conversation system\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}