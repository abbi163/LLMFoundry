{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) with Trae AI\n",
    "\n",
    "This notebook demonstrates how to build a complete RAG system using Trae AI for enhanced retrieval and generation capabilities.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand RAG architecture and components\n",
    "- Set up vector databases for document retrieval\n",
    "- Integrate Trae AI with RAG pipelines\n",
    "- Build an end-to-end RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Vector database and embeddings\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    chroma_available = True\n",
    "except ImportError:\n",
    "    chroma_available = False\n",
    "    print(\"‚ö†Ô∏è  ChromaDB not available. Install with: pip install chromadb\")\n",
    "\n",
    "# Sentence transformers for embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    sentence_transformers_available = True\n",
    "except ImportError:\n",
    "    sentence_transformers_available = False\n",
    "    print(\"‚ö†Ô∏è  SentenceTransformers not available. Install with: pip install sentence-transformers\")\n",
    "\n",
    "# LangChain for RAG pipeline\n",
    "try:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.schema import Document\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "    print(\"‚ö†Ô∏è  LangChain not available. Install with: pip install langchain\")\n",
    "\n",
    "# Standard ML libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(\"Environment setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding RAG Architecture\n",
    "\n",
    "RAG combines retrieval and generation in a two-step process:\n",
    "\n",
    "1. **Retrieval**: Find relevant documents from a knowledge base\n",
    "2. **Generation**: Use retrieved context to generate informed responses\n",
    "\n",
    "### Components:\n",
    "- **Document Store**: Vector database with embedded documents\n",
    "- **Retriever**: Finds relevant documents based on query similarity\n",
    "- **Generator**: LLM that uses retrieved context for response generation\n",
    "- **Embeddings**: Vector representations of text for similarity search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Knowledge Base\n",
    "\n",
    "Let's create a sample knowledge base about AI and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for our knowledge base\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"title\": \"Introduction to Machine Learning\",\n",
    "        \"content\": \"\"\"Machine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves. The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Deep Learning Fundamentals\",\n",
    "        \"content\": \"\"\"Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics and drug design.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Natural Language Processing\",\n",
    "        \"content\": \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Computer Vision Applications\",\n",
    "        \"content\": \"\"\"Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Reinforcement Learning\",\n",
    "        \"content\": \"\"\"Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. Unlike supervised learning, reinforcement learning does not require labelled input/output pairs to be presented, and need not explicitly correct sub-optimal actions.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Transformer Architecture\",\n",
    "        \"content\": \"\"\"The Transformer is a deep learning model introduced in 2017, used primarily in the field of natural language processing (NLP). Like recurrent neural networks (RNNs), Transformers are designed to handle sequential input data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, Transformers do not require that the sequential data be processed in order. This feature allows for much more parallelization than RNNs and therefore reduces training times.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Large Language Models\",\n",
    "        \"content\": \"\"\"Large language models (LLMs) are a type of artificial intelligence model designed to understand and generate human-like text. These models are trained on vast amounts of text data and use deep learning techniques, particularly transformer architectures, to learn patterns in language. LLMs can perform various tasks such as text completion, question answering, summarization, translation, and creative writing. Examples include GPT-3, GPT-4, BERT, and T5.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"AI Ethics and Bias\",\n",
    "        \"content\": \"\"\"AI ethics is a branch of ethics that examines the moral implications of artificial intelligence systems. Key concerns include algorithmic bias, fairness, transparency, accountability, and the potential societal impacts of AI deployment. Bias in AI systems can arise from training data, algorithmic design, or deployment contexts, leading to unfair treatment of certain groups. Addressing these issues requires interdisciplinary collaboration between technologists, ethicists, policymakers, and affected communities.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created knowledge base with {len(sample_documents)} documents\")\n",
    "for doc in sample_documents:\n",
    "    print(f\"- {doc['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Processing and Chunking\n",
    "\n",
    "Split documents into smaller chunks for better retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text splitter (fallback if LangChain not available)\n",
    "class SimpleTextSplitter:\n",
    "    def __init__(self, chunk_size=500, chunk_overlap=50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def split_text(self, text):\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            start = end - self.chunk_overlap\n",
    "        return chunks\n",
    "\n",
    "# Use LangChain splitter if available, otherwise use simple splitter\n",
    "if langchain_available:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "    print(\"‚úÖ Using LangChain RecursiveCharacterTextSplitter\")\n",
    "else:\n",
    "    text_splitter = SimpleTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    print(\"‚úÖ Using simple text splitter\")\n",
    "\n",
    "# Process documents into chunks\n",
    "chunks = []\n",
    "for doc in sample_documents:\n",
    "    if langchain_available:\n",
    "        doc_chunks = text_splitter.split_text(doc['content'])\n",
    "    else:\n",
    "        doc_chunks = text_splitter.split_text(doc['content'])\n",
    "    \n",
    "    for i, chunk in enumerate(doc_chunks):\n",
    "        chunks.append({\n",
    "            'id': f\"{doc['title'].replace(' ', '_').lower()}_{i}\",\n",
    "            'title': doc['title'],\n",
    "            'content': chunk,\n",
    "            'chunk_index': i\n",
    "        })\n",
    "\n",
    "print(f\"\n‚úÖ Created {len(chunks)} text chunks from {len(sample_documents)} documents\")\n",
    "\n",
    "# Show example chunk\n",
    "print(\"\nExample chunk:\")\n",
    "print(f\"Title: {chunks[0]['title']}\")\n",
    "print(f\"Content: {chunks[0]['content'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding Generation\n",
    "\n",
    "Generate embeddings for document chunks using sentence transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple embedding model (fallback)\n",
    "class SimpleEmbedding:\n",
    "    def __init__(self):\n",
    "        # Simple TF-IDF like embedding for demo\n",
    "        self.vocab = {}\n",
    "        self.embedding_dim = 384\n",
    "    \n",
    "    def encode(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            # Simple hash-based embedding for demo\n",
    "            words = text.lower().split()\n",
    "            embedding = np.random.rand(self.embedding_dim)\n",
    "            # Add some consistency based on text content\n",
    "            for word in words:\n",
    "                word_hash = hash(word) % self.embedding_dim\n",
    "                embedding[word_hash] += 0.1\n",
    "            embeddings.append(embedding / np.linalg.norm(embedding))\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "\n",
    "# Load embedding model\n",
    "if sentence_transformers_available:\n",
    "    print(\"Loading SentenceTransformer model...\")\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"‚úÖ SentenceTransformer model loaded\")\n",
    "else:\n",
    "    print(\"Using simple embedding model for demo\")\n",
    "    embedding_model = SimpleEmbedding()\n",
    "    print(\"‚úÖ Simple embedding model initialized\")\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(\"\nGenerating embeddings for document chunks...\")\n",
    "chunk_texts = [chunk['content'] for chunk in chunks]\n",
    "embeddings = embedding_model.encode(chunk_texts)\n",
    "\n",
    "print(f\"‚úÖ Generated embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Add embeddings to chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk['embedding'] = embeddings[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector Database Setup\n",
    "\n",
    "Set up ChromaDB for storing and retrieving document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple vector store (fallback)\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add(self, documents, embeddings, metadata):\n",
    "        self.documents.extend(documents)\n",
    "        self.embeddings.extend(embeddings)\n",
    "        self.metadata.extend(metadata)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = []\n",
    "        for emb in self.embeddings:\n",
    "            sim = np.dot(query_embedding, emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(emb))\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        # Get top k results\n",
    "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'document': self.documents[idx],\n",
    "                'metadata': self.metadata[idx],\n",
    "                'similarity': similarities[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Set up vector store\n",
    "if chroma_available:\n",
    "    print(\"Setting up ChromaDB...\")\n",
    "    \n",
    "    # Initialize ChromaDB client\n",
    "    chroma_client = chromadb.Client()\n",
    "    \n",
    "    # Create or get collection\n",
    "    collection_name = \"ai_knowledge_base\"\n",
    "    try:\n",
    "        collection = chroma_client.create_collection(name=collection_name)\n",
    "    except:\n",
    "        collection = chroma_client.get_collection(name=collection_name)\n",
    "    \n",
    "    # Add documents to collection\n",
    "    collection.add(\n",
    "        documents=[chunk['content'] for chunk in chunks],\n",
    "        embeddings=embeddings.tolist(),\n",
    "        metadatas=[{\n",
    "            'title': chunk['title'],\n",
    "            'chunk_index': chunk['chunk_index'],\n",
    "            'id': chunk['id']\n",
    "        } for chunk in chunks],\n",
    "        ids=[chunk['id'] for chunk in chunks]\n",
    "    )\n",
    "    \n",
    "    vector_store = collection\n",
    "    print(\"‚úÖ ChromaDB collection created and populated\")\n",
    "    \n",
    "else:\n",
    "    print(\"Setting up simple vector store...\")\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add documents\n",
    "    vector_store.add(\n",
    "        documents=[chunk['content'] for chunk in chunks],\n",
    "        embeddings=[chunk['embedding'] for chunk in chunks],\n",
    "        metadata=[{\n",
    "            'title': chunk['title'],\n",
    "            'chunk_index': chunk['chunk_index'],\n",
    "            'id': chunk['id']\n",
    "        } for chunk in chunks]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Simple vector store created and populated\")\n",
    "\n",
    "print(f\"Vector store contains {len(chunks)} document chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieval System\n",
    "\n",
    "Implement the retrieval component of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Retrieval component for RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, embedding_model, top_k=3):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_model = embedding_model\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        # Search vector store\n",
    "        if chroma_available and hasattr(self.vector_store, 'query'):\n",
    "            # ChromaDB query\n",
    "            results = self.vector_store.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=self.top_k\n",
    "            )\n",
    "            \n",
    "            retrieved_docs = []\n",
    "            for i in range(len(results['documents'][0])):\n",
    "                retrieved_docs.append({\n",
    "                    'content': results['documents'][0][i],\n",
    "                    'metadata': results['metadatas'][0][i],\n",
    "                    'distance': results['distances'][0][i] if 'distances' in results else 0\n",
    "                })\n",
    "        else:\n",
    "            # Simple vector store query\n",
    "            results = self.vector_store.similarity_search(query_embedding, k=self.top_k)\n",
    "            retrieved_docs = [{\n",
    "                'content': result['document'],\n",
    "                'metadata': result['metadata'],\n",
    "                'similarity': result['similarity']\n",
    "            } for result in results]\n",
    "        \n",
    "        return retrieved_docs\n",
    "    \n",
    "    def format_context(self, retrieved_docs: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Format retrieved documents into context string\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            title = doc['metadata'].get('title', 'Unknown')\n",
    "            content = doc['content']\n",
    "            context_parts.append(f\"Document {i} ({title}):\\n{content}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = RAGRetriever(vector_store, embedding_model, top_k=3)\n",
    "print(\"‚úÖ RAG retriever initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do transformers work?\",\n",
    "    \"What are the applications of computer vision?\",\n",
    "    \"Explain reinforcement learning\"\n",
    "]\n",
    "\n",
    "print(\"Testing retrieval system:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    retrieved_docs = retriever.retrieve(query)\n",
    "    \n",
    "    print(\"Retrieved documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        title = doc['metadata'].get('title', 'Unknown')\n",
    "        score_key = 'similarity' if 'similarity' in doc else 'distance'\n",
    "        score = doc.get(score_key, 0)\n",
    "        print(f\"  {i}. {title} (score: {score:.3f})\")\n",
    "        print(f\"     {doc['content'][:100]}...\")\n",
    "    \n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generation Component\n",
    "\n",
    "Set up the language model for generating responses based on retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load language model for generation\n",
    "MODEL_NAME = \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "print(f\"Loading generation model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Generation model loaded: {model.num_parameters():,} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGGenerator:\n",
    "    \"\"\"Generation component for RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, max_length=512):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def generate_response(self, query: str, context: str) -> str:\n",
    "        \"\"\"Generate response using query and retrieved context\"\"\"\n",
    "        # Create prompt with context and query\n",
    "        prompt = self._create_prompt(query, context)\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors='pt', max_length=self.max_length, truncation=True)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=inputs.shape[1] + 150,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _create_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Create a well-formatted prompt for generation\"\"\"\n",
    "        prompt = f\"\"\"Context Information:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Based on the context information above, please provide a comprehensive answer to the question:\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "# Initialize generator\n",
    "generator = RAGGenerator(model, tokenizer)\n",
    "print(\"‚úÖ RAG generator initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete RAG Pipeline\n",
    "\n",
    "Combine retrieval and generation into a complete RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraeRAGPipeline:\n",
    "    \"\"\"Complete RAG pipeline with Trae AI optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever, generator):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def query(self, question: str, include_sources: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Process a query through the complete RAG pipeline\"\"\"\n",
    "        # Step 1: Retrieve relevant documents\n",
    "        retrieved_docs = self.retriever.retrieve(question)\n",
    "        \n",
    "        # Step 2: Format context\n",
    "        context = self.retriever.format_context(retrieved_docs)\n",
    "        \n",
    "        # Step 3: Generate response\n",
    "        response = self.generator.generate_response(question, context)\n",
    "        \n",
    "        # Step 4: Prepare result\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'answer': response,\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'context_used': context\n",
    "        }\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def format_response(self, result: Dict[str, Any], include_sources: bool = True) -> str:\n",
    "        \"\"\"Format the response for display\"\"\"\n",
    "        formatted = f\"**Question:** {result['question']}\\n\\n\"\n",
    "        formatted += f\"**Answer:** {result['answer']}\\n\\n\"\n",
    "        \n",
    "        if include_sources:\n",
    "            formatted += \"**Sources:**\\n\"\n",
    "            for i, doc in enumerate(result['retrieved_docs'], 1):\n",
    "                title = doc['metadata'].get('title', 'Unknown')\n",
    "                formatted += f\"{i}. {title}\\n\"\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def get_conversation_history(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get the conversation history\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Initialize complete RAG pipeline\n",
    "rag_pipeline = TraeRAGPipeline(retriever, generator)\n",
    "print(\"‚úÖ Complete RAG pipeline initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Testing the RAG System\n",
    "\n",
    "Test our complete RAG system with various questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is machine learning and how does it work?\",\n",
    "    \"Can you explain the transformer architecture?\",\n",
    "    \"What are the main applications of computer vision?\",\n",
    "    \"How does reinforcement learning differ from supervised learning?\",\n",
    "    \"What are the ethical concerns with AI systems?\"\n",
    "]\n",
    "\n",
    "print(\"Testing complete RAG pipeline:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\nü§î Question {i}: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get response from RAG pipeline\n",
    "    result = rag_pipeline.query(question)\n",
    "    \n",
    "    # Display formatted response\n",
    "    print(f\"ü§ñ Answer: {result['answer']}\")\n",
    "    \n",
    "    print(\"üìö Sources used:\")\n",
    "    for j, doc in enumerate(result['retrieved_docs'], 1):\n",
    "        title = doc['metadata'].get('title', 'Unknown')\n",
    "        score_key = 'similarity' if 'similarity' in doc else 'distance'\n",
    "        score = doc.get(score_key, 0)\n",
    "        print(f\"   {j}. {title} (relevance: {score:.3f})\")\n",
    "    \n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. RAG System Evaluation\n",
    "\n",
    "Evaluate the performance of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate RAG system performance\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_pipeline):\n",
    "        self.rag_pipeline = rag_pipeline\n",
    "    \n",
    "    def evaluate_retrieval(self, questions_and_expected_docs):\n",
    "        \"\"\"Evaluate retrieval accuracy\"\"\"\n",
    "        total_questions = len(questions_and_expected_docs)\n",
    "        correct_retrievals = 0\n",
    "        \n",
    "        for question, expected_titles in questions_and_expected_docs:\n",
    "            retrieved_docs = self.rag_pipeline.retriever.retrieve(question)\n",
    "            retrieved_titles = [doc['metadata']['title'] for doc in retrieved_docs]\n",
    "            \n",
    "            # Check if any expected document was retrieved\n",
    "            if any(title in retrieved_titles for title in expected_titles):\n",
    "                correct_retrievals += 1\n",
    "        \n",
    "        accuracy = correct_retrievals / total_questions\n",
    "        return accuracy\n",
    "    \n",
    "    def evaluate_response_quality(self, questions):\n",
    "        \"\"\"Evaluate response quality metrics\"\"\"\n",
    "        metrics = {\n",
    "            'avg_response_length': 0,\n",
    "            'avg_retrieval_time': 0,\n",
    "            'avg_generation_time': 0\n",
    "        }\n",
    "        \n",
    "        total_response_length = 0\n",
    "        \n",
    "        for question in questions:\n",
    "            import time\n",
    "            \n",
    "            # Measure retrieval time\n",
    "            start_time = time.time()\n",
    "            retrieved_docs = self.rag_pipeline.retriever.retrieve(question)\n",
    "            retrieval_time = time.time() - start_time\n",
    "            \n",
    "            # Measure generation time\n",
    "            context = self.rag_pipeline.retriever.format_context(retrieved_docs)\n",
    "            start_time = time.time()\n",
    "            response = self.rag_pipeline.generator.generate_response(question, context)\n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            total_response_length += len(response.split())\n",
    "            metrics['avg_retrieval_time'] += retrieval_time\n",
    "            metrics['avg_generation_time'] += generation_time\n",
    "        \n",
    "        # Calculate averages\n",
    "        num_questions = len(questions)\n",
    "        metrics['avg_response_length'] = total_response_length / num_questions\n",
    "        metrics['avg_retrieval_time'] /= num_questions\n",
    "        metrics['avg_generation_time'] /= num_questions\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Evaluation data\n",
    "eval_questions_and_docs = [\n",
    "    (\"What is machine learning?\", [\"Introduction to Machine Learning\"]),\n",
    "    (\"How do transformers work?\", [\"Transformer Architecture\"]),\n",
    "    (\"What is computer vision?\", [\"Computer Vision Applications\"]),\n",
    "    (\"Explain reinforcement learning\", [\"Reinforcement Learning\"]),\n",
    "    (\"What are large language models?\", [\"Large Language Models\"])\n",
    "]\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RAGEvaluator(rag_pipeline)\n",
    "\n",
    "# Evaluate retrieval accuracy\n",
    "retrieval_accuracy = evaluator.evaluate_retrieval(eval_questions_and_docs)\n",
    "print(f\"üìä Retrieval Accuracy: {retrieval_accuracy:.2%}\")\n",
    "\n",
    "# Evaluate response quality\n",
    "eval_questions = [q for q, _ in eval_questions_and_docs]\n",
    "quality_metrics = evaluator.evaluate_response_quality(eval_questions)\n",
    "\n",
    "print(\"\nüìà Performance Metrics:\")\n",
    "print(f\"  Average response length: {quality_metrics['avg_response_length']:.1f} words\")\n",
    "print(f\"  Average retrieval time: {quality_metrics['avg_retrieval_time']:.3f} seconds\")\n",
    "print(f\"  Average generation time: {quality_metrics['avg_generation_time']:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Trae AI Integration and Optimizations\n",
    "\n",
    "Demonstrate how Trae AI can enhance the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trae AI enhanced RAG pipeline\n",
    "class TraeEnhancedRAG(TraeRAGPipeline):\n",
    "    \"\"\"RAG pipeline with Trae AI optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever, generator):\n",
    "        super().__init__(retriever, generator)\n",
    "        self.trae_optimizations = {\n",
    "            'adaptive_retrieval': True,\n",
    "            'context_compression': True,\n",
    "            'response_caching': True,\n",
    "            'quality_filtering': True\n",
    "        }\n",
    "        self.response_cache = {}\n",
    "    \n",
    "    def query(self, question: str, include_sources: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced query with Trae AI optimizations\"\"\"\n",
    "        # Check cache first\n",
    "        if self.trae_optimizations['response_caching']:\n",
    "            cache_key = hash(question)\n",
    "            if cache_key in self.response_cache:\n",
    "                print(\"üöÄ Using cached response\")\n",
    "                return self.response_cache[cache_key]\n",
    "        \n",
    "        # Adaptive retrieval - adjust number of documents based on query complexity\n",
    "        if self.trae_optimizations['adaptive_retrieval']:\n",
    "            query_complexity = len(question.split())\n",
    "            if query_complexity > 10:\n",
    "                self.retriever.top_k = 5  # More documents for complex queries\n",
    "            else:\n",
    "                self.retriever.top_k = 3  # Fewer for simple queries\n",
    "        \n",
    "        # Standard retrieval\n",
    "        retrieved_docs = self.retriever.retrieve(question)\n",
    "        \n",
    "        # Quality filtering - remove low-relevance documents\n",
    "        if self.trae_optimizations['quality_filtering']:\n",
    "            threshold = 0.3  # Minimum similarity threshold\n",
    "            retrieved_docs = [\n",
    "                doc for doc in retrieved_docs \n",
    "                if doc.get('similarity', doc.get('distance', 1)) > threshold\n",
    "            ]\n",
    "        \n",
    "        # Context compression - optimize context length\n",
    "        if self.trae_optimizations['context_compression']:\n",
    "            context = self._compress_context(retrieved_docs, question)\n",
    "        else:\n",
    "            context = self.retriever.format_context(retrieved_docs)\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generator.generate_response(question, context)\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'answer': response,\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'context_used': context,\n",
    "            'trae_optimizations_used': self.trae_optimizations\n",
    "        }\n",
    "        \n",
    "        # Cache result\n",
    "        if self.trae_optimizations['response_caching']:\n",
    "            self.response_cache[cache_key] = result\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _compress_context(self, retrieved_docs, question):\n",
    "        \"\"\"Compress context by extracting most relevant sentences\"\"\"\n",
    "        # Simple compression: take first 2 sentences from each document\n",
    "        compressed_parts = []\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            title = doc['metadata'].get('title', 'Unknown')\n",
    "            content = doc['content']\n",
    "            \n",
    "            # Extract first 2 sentences\n",
    "            sentences = content.split('. ')\n",
    "            compressed_content = '. '.join(sentences[:2])\n",
    "            if not compressed_content.endswith('.'):\n",
    "                compressed_content += '.'\n",
    "            \n",
    "            compressed_parts.append(f\"Source {i} ({title}): {compressed_content}\")\n",
    "        \n",
    "        return '\\n\\n'.join(compressed_parts)\n",
    "\n",
    "# Initialize Trae-enhanced RAG\n",
    "trae_rag = TraeEnhancedRAG(retriever, generator)\n",
    "print(\"‚úÖ Trae AI enhanced RAG pipeline initialized\")\n",
    "\n",
    "# Test enhanced pipeline\n",
    "print(\"\nüöÄ Testing Trae AI enhanced RAG:\")\n",
    "test_question = \"What is the difference between machine learning and deep learning?\"\n",
    "\n",
    "result = trae_rag.query(test_question)\n",
    "print(f\"\nQuestion: {result['question']}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"\nOptimizations used: {result['trae_optimizations_used']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Saving and Deployment\n",
    "\n",
    "Save the RAG system components for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RAG system configuration\n",
    "rag_config = {\n",
    "    'embedding_model': 'all-MiniLM-L6-v2' if sentence_transformers_available else 'simple',\n",
    "    'generation_model': MODEL_NAME,\n",
    "    'vector_store': 'chromadb' if chroma_available else 'simple',\n",
    "    'retrieval_top_k': 3,\n",
    "    'max_generation_length': 150,\n",
    "    'trae_optimizations': trae_rag.trae_optimizations\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = Path('../trae_llm/rag_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(rag_config, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ RAG configuration saved to {config_path}\")\n",
    "\n",
    "# Save sample knowledge base\n",
    "kb_path = Path('../data/knowledge_base.json')\n",
    "with open(kb_path, 'w') as f:\n",
    "    json.dump(sample_documents, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Knowledge base saved to {kb_path}\")\n",
    "\n",
    "# Performance summary\n",
    "print(\"\nüìä RAG System Summary:\")\n",
    "print(f\"  Documents in knowledge base: {len(sample_documents)}\")\n",
    "print(f\"  Text chunks created: {len(chunks)}\")\n",
    "print(f\"  Embedding model: {rag_config['embedding_model']}\")\n",
    "print(f\"  Generation model: {rag_config['generation_model']}\")\n",
    "print(f\"  Vector store: {rag_config['vector_store']}\")\n",
    "print(f\"  Retrieval accuracy: {retrieval_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations! You've built a complete RAG system with Trae AI enhancements. Continue with:\n",
    "\n",
    "- **05_evaluation_and_visualization.ipynb**: Comprehensive evaluation and visualization\n",
    "- **scripts/launch_rag_server.py**: Deploy your RAG system as an API\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **RAG Architecture**: Combines retrieval and generation for informed responses\n",
    "2. **Vector Databases**: Enable efficient similarity search for document retrieval\n",
    "3. **Trae AI Optimizations**: Enhance performance with adaptive retrieval and context compression\n",
    "4. **Evaluation**: Important to measure both retrieval accuracy and response quality\n",
    "\n",
    "## Advanced Exercises\n",
    "\n",
    "Try these advanced features:\n",
    "\n",
    "1. **Multi-modal RAG**: Add support for images and documents\n",
    "2. **Hybrid Search**: Combine semantic and keyword search\n",
    "3. **Dynamic Knowledge Base**: Add real-time document updates\n",
    "4. **Conversation Memory**: Maintain context across multiple queries\n",
    "5. **Custom Embeddings**: Fine-tune embeddings for your domain\n",
    "6. **Advanced Filtering**: Implement metadata-based filtering\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}