{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Evaluation and Visualization\n",
    "\n",
    "This notebook provides comprehensive tools for evaluating and visualizing LLM performance.\n",
    "\n",
    "## Topics Covered:\n",
    "- Model evaluation metrics\n",
    "- Performance visualization\n",
    "- Model comparison\n",
    "- Advanced evaluation techniques\n",
    "- Report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Framework Setup\n",
    "\n",
    "Create a comprehensive evaluation framework for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEvaluator:\n",
    "    \"\"\"Comprehensive LLM evaluation framework\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, device: str = 'auto'):\n",
    "        self.model_name = model_name\n",
    "        self.device = self._get_device(device)\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.evaluation_results = {}\n",
    "    \n",
    "    def _get_device(self, device: str) -> str:\n",
    "        if device == 'auto':\n",
    "            return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        return device\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model and tokenizer\"\"\"\n",
    "        print(f'Loading model: {self.model_name}')\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n",
    "            device_map='auto' if self.device == 'cuda' else None\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(f'Model loaded on {self.device}')\n",
    "    \n",
    "    def calculate_perplexity(self, texts: List[str]) -> float:\n",
    "        \"\"\"Calculate perplexity on a list of texts\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            raise ValueError('Model not loaded. Call load_model() first.')\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = self.model(**inputs, labels=inputs['input_ids'])\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                total_loss += loss.item() * inputs['input_ids'].size(1)\n",
    "                total_tokens += inputs['input_ids'].size(1)\n",
    "        \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    def generate_text(self, prompts: List[str], max_length: int = 100, \n",
    "                     num_return_sequences: int = 1) -> List[str]:\n",
    "        \"\"\"Generate text from prompts\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            raise ValueError('Model not loaded. Call load_model() first.')\n",
    "        \n",
    "        generations = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        for prompt in prompts:\n",
    "            inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length,\n",
    "                    num_return_sequences=num_return_sequences,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            for output in outputs:\n",
    "                generated_text = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "                # Remove the original prompt from the generated text\n",
    "                generated_text = generated_text[len(prompt):].strip()\n",
    "                generations.append(generated_text)\n",
    "        \n",
    "        return generations\n",
    "    \n",
    "    def measure_inference_speed(self, prompts: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Measure inference speed metrics\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            raise ValueError('Model not loaded. Call load_model() first.')\n",
    "        \n",
    "        times = []\n",
    "        token_counts = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        for prompt in prompts:\n",
    "            inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=100,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            inference_time = end_time - start_time\n",
    "            token_count = outputs[0].size(0)\n",
    "            \n",
    "            times.append(inference_time)\n",
    "            token_counts.append(token_count)\n",
    "        \n",
    "        return {\n",
    "            'avg_time_per_sample': np.mean(times),\n",
    "            'avg_tokens_per_second': np.mean(token_counts) / np.mean(times),\n",
    "            'total_time': sum(times),\n",
    "            'total_tokens': sum(token_counts)\n",
    "        }\n",
    "\n",
    "print('LLMEvaluator class defined successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data and Model Setup\n",
    "\n",
    "Set up sample data and load a model for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample evaluation data\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is transforming how we process data.\",\n",
    "    \"Natural language processing enables computers to understand text.\",\n",
    "    \"Deep learning models require large amounts of training data.\",\n",
    "    \"Artificial intelligence will shape the future of technology.\"\n",
    "]\n",
    "\n",
    "sample_prompts = [\n",
    "    \"Explain machine learning in simple terms:\",\n",
    "    \"What is the future of AI?\",\n",
    "    \"How does natural language processing work?\",\n",
    "    \"Describe the benefits of deep learning:\"\n",
    "]\n",
    "\n",
    "# Initialize evaluator with a smaller model for demonstration\n",
    "evaluator = LLMEvaluator('gpt2')\n",
    "\n",
    "print('Sample data prepared!')\n",
    "print(f'Sample texts: {len(sample_texts)}')\n",
    "print(f'Sample prompts: {len(sample_prompts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "evaluator.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "Perform comprehensive model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity\n",
    "print('Calculating perplexity...')\n",
    "perplexity = evaluator.calculate_perplexity(sample_texts)\n",
    "print(f'Perplexity: {perplexity:.2f}')\n",
    "\n",
    "# Generate text samples\n",
    "print('\\nGenerating text samples...')\n",
    "generations = evaluator.generate_text(sample_prompts, max_length=80)\n",
    "\n",
    "for i, (prompt, generation) in enumerate(zip(sample_prompts, generations)):\n",
    "    print(f'\\nPrompt {i+1}: {prompt}')\n",
    "    print(f'Generation: {generation}')\n",
    "\n",
    "# Measure inference speed\n",
    "print('\\nMeasuring inference speed...')\n",
    "speed_metrics = evaluator.measure_inference_speed(sample_prompts[:2])  # Use fewer samples for speed\n",
    "\n",
    "for metric, value in speed_metrics.items():\n",
    "    print(f'{metric}: {value:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Dashboard\n",
    "\n",
    "Create interactive visualizations for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationVisualizer:\n",
    "    \"\"\"Create visualizations for model evaluation results\"\"\"\n",
    "    \n",
    "    def __init__(self, figsize: tuple = (12, 8)):\n",
    "        self.figsize = figsize\n",
    "        plt.rcParams['figure.figsize'] = figsize\n",
    "    \n",
    "    def plot_metrics_overview(self, metrics: Dict[str, float], title: str = 'Model Performance Metrics'):\n",
    "        \"\"\"Plot overview of key metrics\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=self.figsize)\n",
    "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Perplexity gauge\n",
    "        perplexity = metrics.get('perplexity', 0)\n",
    "        ax1.bar(['Perplexity'], [perplexity], color='skyblue')\n",
    "        ax1.set_title('Perplexity (Lower is Better)')\n",
    "        ax1.set_ylabel('Score')\n",
    "        \n",
    "        # Speed metrics\n",
    "        speed_data = {\n",
    "            'Tokens/sec': metrics.get('avg_tokens_per_second', 0),\n",
    "            'Time/sample': metrics.get('avg_time_per_sample', 0)\n",
    "        }\n",
    "        ax2.bar(speed_data.keys(), speed_data.values(), color=['lightcoral', 'lightgreen'])\n",
    "        ax2.set_title('Inference Speed')\n",
    "        ax2.set_ylabel('Value')\n",
    "        \n",
    "        # Token distribution\n",
    "        token_counts = [len(text.split()) for text in sample_texts]\n",
    "        ax3.hist(token_counts, bins=10, color='gold', alpha=0.7)\n",
    "        ax3.set_title('Token Count Distribution')\n",
    "        ax3.set_xlabel('Token Count')\n",
    "        ax3.set_ylabel('Frequency')\n",
    "        \n",
    "        # Performance radar (placeholder)\n",
    "        categories = ['Speed', 'Quality', 'Efficiency', 'Consistency']\n",
    "        values = [0.8, 0.7, 0.9, 0.6]  # Example values\n",
    "        \n",
    "        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)\n",
    "        values += values[:1]  # Complete the circle\n",
    "        angles = np.concatenate((angles, [angles[0]]))\n",
    "        \n",
    "        ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
    "        ax4.plot(angles, values, 'o-', linewidth=2, color='purple')\n",
    "        ax4.fill(angles, values, alpha=0.25, color='purple')\n",
    "        ax4.set_xticks(angles[:-1])\n",
    "        ax4.set_xticklabels(categories)\n",
    "        ax4.set_title('Performance Radar')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_generation_analysis(self, prompts: List[str], generations: List[str]):\n",
    "        \"\"\"Analyze generated text characteristics\"\"\"\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Generation length distribution\n",
    "        gen_lengths = [len(gen.split()) for gen in generations]\n",
    "        ax1.hist(gen_lengths, bins=10, color='lightblue', alpha=0.7)\n",
    "        ax1.set_title('Generation Length Distribution')\n",
    "        ax1.set_xlabel('Word Count')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        \n",
    "        # Prompt vs Generation length\n",
    "        prompt_lengths = [len(prompt.split()) for prompt in prompts]\n",
    "        ax2.scatter(prompt_lengths, gen_lengths, color='red', alpha=0.6)\n",
    "        ax2.set_title('Prompt vs Generation Length')\n",
    "        ax2.set_xlabel('Prompt Length (words)')\n",
    "        ax2.set_ylabel('Generation Length (words)')\n",
    "        \n",
    "        # Average word length in generations\n",
    "        avg_word_lengths = [np.mean([len(word) for word in gen.split()]) for gen in generations]\n",
    "        ax3.bar(range(len(avg_word_lengths)), avg_word_lengths, color='green', alpha=0.7)\n",
    "        ax3.set_title('Average Word Length per Generation')\n",
    "        ax3.set_xlabel('Generation Index')\n",
    "        ax3.set_ylabel('Average Word Length')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_training_dynamics(self, epochs: List[int], train_loss: List[float], \n",
    "                              val_loss: List[float] = None):\n",
    "        \"\"\"Plot training dynamics (for fine-tuning scenarios)\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        plt.plot(epochs, train_loss, 'b-', label='Training Loss', linewidth=2)\n",
    "        \n",
    "        if val_loss:\n",
    "            plt.plot(epochs, val_loss, 'r-', label='Validation Loss', linewidth=2)\n",
    "        \n",
    "        plt.title('Training Dynamics', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Create visualizer and generate plots\n",
    "visualizer = EvaluationVisualizer()\n",
    "\n",
    "# Prepare metrics for visualization\n",
    "metrics_for_viz = {\n",
    "    'perplexity': perplexity,\n",
    "    **speed_metrics\n",
    "}\n",
    "\n",
    "# Generate visualizations\n",
    "visualizer.plot_metrics_overview(metrics_for_viz, 'GPT-2 Model Performance')\n",
    "visualizer.plot_generation_analysis(sample_prompts, generations)\n",
    "\n",
    "# Example training dynamics\n",
    "example_epochs = list(range(1, 11))\n",
    "example_train_loss = [2.5 - 0.2*i + 0.1*np.random.randn() for i in example_epochs]\n",
    "example_val_loss = [2.7 - 0.15*i + 0.15*np.random.randn() for i in example_epochs]\n",
    "\n",
    "visualizer.plot_training_dynamics(example_epochs, example_train_loss, example_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison Framework\n",
    "\n",
    "Compare multiple models side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparator:\n",
    "    \"\"\"Compare multiple models across various metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def add_model_results(self, model_name: str, results: Dict[str, Any]):\n",
    "        \"\"\"Add evaluation results for a model\"\"\"\n",
    "        self.results[model_name] = results\n",
    "    \n",
    "    def compare_metrics(self, metrics: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Create comparison dataframe for specified metrics\"\"\"\n",
    "        comparison_data = {}\n",
    "        \n",
    "        for model_name, results in self.results.items():\n",
    "            comparison_data[model_name] = [results.get(metric, 0) for metric in metrics]\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data, index=metrics)\n",
    "        return df\n",
    "    \n",
    "    def plot_comparison(self, metrics: List[str], title: str = 'Model Comparison'):\n",
    "        \"\"\"Plot model comparison\"\"\"\n",
    "        df = self.compare_metrics(metrics)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Bar plot\n",
    "        df.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "        axes[0].set_title(f'{title} - Bar Chart')\n",
    "        axes[0].set_ylabel('Score')\n",
    "        axes[0].legend(title='Models')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Heatmap\n",
    "        sns.heatmap(df, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=axes[1])\n",
    "        axes[1].set_title(f'{title} - Heatmap')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def rank_models(self, metrics: List[str], weights: List[float] = None) -> pd.DataFrame:\n",
    "        \"\"\"Rank models based on weighted metrics\"\"\"\n",
    "        df = self.compare_metrics(metrics)\n",
    "        \n",
    "        if weights is None:\n",
    "            weights = [1.0] * len(metrics)\n",
    "        \n",
    "        # Normalize metrics (assuming higher is better, except for perplexity)\n",
    "        normalized_df = df.copy()\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if 'perplexity' in metric.lower() or 'loss' in metric.lower():\n",
    "                # Lower is better - invert\n",
    "                normalized_df.loc[metric] = 1 / (df.loc[metric] + 1e-6)\n",
    "            \n",
    "            # Min-max normalization\n",
    "            min_val = normalized_df.loc[metric].min()\n",
    "            max_val = normalized_df.loc[metric].max()\n",
    "            if max_val > min_val:\n",
    "                normalized_df.loc[metric] = (normalized_df.loc[metric] - min_val) / (max_val - min_val)\n",
    "        \n",
    "        # Calculate weighted scores\n",
    "        weighted_scores = {}\n",
    "        for model in df.columns:\n",
    "            score = sum(normalized_df.loc[metric, model] * weight \n",
    "                       for metric, weight in zip(metrics, weights))\n",
    "            weighted_scores[model] = score / sum(weights)\n",
    "        \n",
    "        # Create ranking dataframe\n",
    "        ranking_df = pd.DataFrame(list(weighted_scores.items()), \n",
    "                                 columns=['Model', 'Weighted Score'])\n",
    "        ranking_df = ranking_df.sort_values('Weighted Score', ascending=False)\n",
    "        ranking_df['Rank'] = range(1, len(ranking_df) + 1)\n",
    "        \n",
    "        return ranking_df[['Rank', 'Model', 'Weighted Score']]\n",
    "\n",
    "# Example model comparison\n",
    "comparator = ModelComparator()\n",
    "\n",
    "# Add results for multiple models (simulated)\n",
    "comparator.add_model_results('GPT-2', {\n",
    "    'perplexity': perplexity,\n",
    "    'avg_tokens_per_second': speed_metrics['avg_tokens_per_second'],\n",
    "    'avg_time_per_sample': speed_metrics['avg_time_per_sample']\n",
    "})\n",
    "\n",
    "# Simulated results for other models\n",
    "comparator.add_model_results('GPT-2-Medium', {\n",
    "    'perplexity': perplexity * 0.8,\n",
    "    'avg_tokens_per_second': speed_metrics['avg_tokens_per_second'] * 0.7,\n",
    "    'avg_time_per_sample': speed_metrics['avg_time_per_sample'] * 1.3\n",
    "})\n",
    "\n",
    "comparator.add_model_results('DistilGPT-2', {\n",
    "    'perplexity': perplexity * 1.2,\n",
    "    'avg_tokens_per_second': speed_metrics['avg_tokens_per_second'] * 1.5,\n",
    "    'avg_time_per_sample': speed_metrics['avg_time_per_sample'] * 0.6\n",
    "})\n",
    "\n",
    "# Compare models\n",
    "metrics_to_compare = ['perplexity', 'avg_tokens_per_second', 'avg_time_per_sample']\n",
    "comparison_df = comparator.plot_comparison(metrics_to_compare, 'GPT Model Comparison')\n",
    "\n",
    "print('\\nComparison DataFrame:')\n",
    "print(comparison_df)\n",
    "\n",
    "# Rank models\n",
    "ranking = comparator.rank_models(metrics_to_compare, weights=[0.4, 0.3, 0.3])\n",
    "print('\\nModel Ranking:')\n",
    "print(ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Evaluation Metrics\n",
    "\n",
    "Implement advanced metrics for comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMetrics:\n",
    "    \"\"\"Advanced evaluation metrics for LLMs\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_bleu_score(reference: str, candidate: str, n: int = 4) -> float:\n",
    "        \"\"\"Calculate BLEU score (simplified implementation)\"\"\"\n",
    "        ref_words = reference.lower().split()\n",
    "        cand_words = candidate.lower().split()\n",
    "        \n",
    "        if len(cand_words) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate n-gram precision\n",
    "        precisions = []\n",
    "        \n",
    "        for i in range(1, min(n + 1, len(cand_words) + 1)):\n",
    "            ref_ngrams = [tuple(ref_words[j:j+i]) for j in range(len(ref_words) - i + 1)]\n",
    "            cand_ngrams = [tuple(cand_words[j:j+i]) for j in range(len(cand_words) - i + 1)]\n",
    "            \n",
    "            if len(cand_ngrams) == 0:\n",
    "                precisions.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            matches = sum(1 for ngram in cand_ngrams if ngram in ref_ngrams)\n",
    "            precision = matches / len(cand_ngrams)\n",
    "            precisions.append(precision)\n",
    "        \n",
    "        if not precisions or all(p == 0 for p in precisions):\n",
    "            return 0.0\n",
    "        \n",
    "        # Geometric mean of precisions\n",
    "        bleu = np.exp(np.mean([np.log(p) if p > 0 else -float('inf') for p in precisions]))\n",
    "        \n",
    "        # Brevity penalty\n",
    "        bp = min(1.0, np.exp(1 - len(ref_words) / len(cand_words)))\n",
    "        \n",
    "        return bp * bleu\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rouge_l(reference: str, candidate: str) -> float:\n",
    "        \"\"\"Simple ROUGE-L score implementation\"\"\"\n",
    "        ref_words = reference.lower().split()\n",
    "        cand_words = candidate.lower().split()\n",
    "        \n",
    "        if not ref_words or not cand_words:\n",
    "            return 0.0\n",
    "        \n",
    "        # Find longest common subsequence\n",
    "        def lcs_length(x, y):\n",
    "            m, n = len(x), len(y)\n",
    "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "            \n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if x[i-1] == y[j-1]:\n",
    "                        dp[i][j] = dp[i-1][j-1] + 1\n",
    "                    else:\n",
    "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "            \n",
    "            return dp[m][n]\n",
    "        \n",
    "        lcs_len = lcs_length(ref_words, cand_words)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        precision = lcs_len / len(cand_words)\n",
    "        recall = lcs_len / len(ref_words)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        return f1\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_semantic_similarity(text1: str, text2: str) -> float:\n",
    "        \"\"\"Simple semantic similarity based on word overlap\"\"\"\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_readability_score(text: str) -> float:\n",
    "        \"\"\"Simple readability score based on sentence and word length\"\"\"\n",
    "        sentences = text.split('.')\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if not sentences:\n",
    "            return 0.0\n",
    "        \n",
    "        words = text.split()\n",
    "        \n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "        \n",
    "        # Simple readability formula (lower is more readable)\n",
    "        readability = 206.835 - 1.015 * avg_sentence_length - 84.6 * (avg_word_length / 4.7)\n",
    "        \n",
    "        # Normalize to 0-1 range\n",
    "        return max(0, min(1, readability / 100))\n",
    "\n",
    "# Test advanced metrics\n",
    "print('Testing advanced evaluation metrics...')\n",
    "\n",
    "# Sample texts for testing\n",
    "reference_text = \"Machine learning is a powerful tool for data analysis and prediction.\"\n",
    "candidate_text = \"Machine learning provides powerful tools for analyzing data and making predictions.\"\n",
    "\n",
    "metrics = AdvancedMetrics()\n",
    "\n",
    "bleu = metrics.calculate_bleu_score(reference_text, candidate_text)\n",
    "rouge = metrics.calculate_rouge_l(reference_text, candidate_text)\n",
    "similarity = metrics.calculate_semantic_similarity(reference_text, candidate_text)\n",
    "readability = metrics.calculate_readability_score(candidate_text)\n",
    "\n",
    "print(f'BLEU Score: {bleu:.3f}')\n",
    "print(f'ROUGE-L Score: {rouge:.3f}')\n",
    "print(f'Semantic Similarity: {similarity:.3f}')\n",
    "print(f'Readability Score: {readability:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Report Generation\n",
    "\n",
    "Generate comprehensive evaluation reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationReporter:\n",
    "    \"\"\"Generate comprehensive evaluation reports\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, results: Dict[str, Any]):\n",
    "        self.model_name = model_name\n",
    "        self.results = results\n",
    "        self.timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    def generate_html_report(self) -> str:\n",
    "        \"\"\"Generate HTML evaluation report\"\"\"\n",
    "        html = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Model Evaluation Report - {self.model_name}</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}\n",
    "                .metric {{ margin: 10px 0; padding: 10px; background-color: #f9f9f9; border-left: 4px solid #007acc; }}\n",
    "                .generation {{ margin: 10px 0; padding: 15px; background-color: #fff; border: 1px solid #ddd; border-radius: 5px; }}\n",
    "                .prompt {{ font-weight: bold; color: #333; }}\n",
    "                .response {{ margin-top: 10px; color: #666; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"header\">\n",
    "                <h1>Model Evaluation Report</h1>\n",
    "                <p><strong>Model:</strong> {self.model_name}</p>\n",
    "                <p><strong>Evaluation Date:</strong> {self.timestamp}</p>\n",
    "            </div>\n",
    "            \n",
    "            <h2>Performance Metrics</h2>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add metrics\n",
    "        for metric, value in self.results.items():\n",
    "            if metric not in ['generations', 'retrieved_docs', 'context_used']:\n",
    "                if isinstance(value, float):\n",
    "                    html += f'<div class=\"metric\"><strong>{metric.replace(\"_\", \" \").title()}:</strong> {value:.3f}</div>\\n'\n",
    "                else:\n",
    "                    html += f'<div class=\"metric\"><strong>{metric.replace(\"_\", \" \").title()}:</strong> {value}</div>\\n'\n",
    "        \n",
    "        # Add generations if available\n",
    "        if 'generations' in self.results:\n",
    "            html += \"<h2>Sample Generations</h2>\\n\"\n",
    "            for i, generation in enumerate(self.results['generations'][:5], 1):\n",
    "                html += f\"\"\"\n",
    "                <div class=\"generation\">\n",
    "                    <div class=\"prompt\">Generation {i}:</div>\n",
    "                    <div class=\"response\">{generation}</div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def generate_markdown_report(self) -> str:\n",
    "        \"\"\"Generate Markdown evaluation report\"\"\"\n",
    "        report = f\"\"\"# Model Evaluation Report\n",
    "\n",
    "**Model:** {self.model_name}\n",
    "**Evaluation Date:** {self.timestamp}\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Add metrics table\n",
    "        report += \"| Metric | Value |\\n|--------|-------|\\n\"\n",
    "        \n",
    "        for metric, value in self.results.items():\n",
    "            if metric not in ['generations', 'retrieved_docs', 'context_used']:\n",
    "                metric_name = metric.replace('_', ' ').title()\n",
    "                if isinstance(value, float):\n",
    "                    report += f\"| {metric_name} | {value:.3f} |\\n\"\n",
    "                else:\n",
    "                    report += f\"| {metric_name} | {value} |\\n\"\n",
    "        \n",
    "        # Add generations\n",
    "        if 'generations' in self.results:\n",
    "            report += \"\\n## Sample Generations\\n\\n\"\n",
    "            for i, generation in enumerate(self.results['generations'][:3], 1):\n",
    "                report += f\"### Generation {i}\\n\\n{generation}\\n\\n\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def save_report(self, format_type: str = 'markdown', output_dir: str = '../logs') -> str:\n",
    "        \"\"\"Save evaluation report to file\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        timestamp_str = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        if format_type == 'html':\n",
    "            filename = f\"evaluation_report_{self.model_name}_{timestamp_str}.html\"\n",
    "            content = self.generate_html_report()\n",
    "        else:\n",
    "            filename = f\"evaluation_report_{self.model_name}_{timestamp_str}.md\"\n",
    "            content = self.generate_markdown_report()\n",
    "        \n",
    "        file_path = output_path / filename\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        return str(file_path)\n",
    "\n",
    "# Generate and save evaluation reports\n",
    "print('Generating evaluation reports...')\n",
    "\n",
    "# Create sample results for demonstration\n",
    "sample_results = {\n",
    "    'perplexity': 15.2,\n",
    "    'bleu_score': 0.65,\n",
    "    'rouge_l': 0.72,\n",
    "    'semantic_similarity': 0.78,\n",
    "    'response_time_ms': 245,\n",
    "    'total_tokens': 1250,\n",
    "    'generations': [\n",
    "        \"Machine learning enables computers to learn patterns from data without explicit programming.\",\n",
    "        \"Deep learning uses neural networks with multiple layers to process complex information.\",\n",
    "        \"Natural language processing helps computers understand and generate human language.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create reporter and generate reports\n",
    "reporter = EvaluationReporter(\"gpt2-medium\", sample_results)\n",
    "\n",
    "# Generate and save markdown report\n",
    "md_path = reporter.save_report('markdown')\n",
    "print(f'Markdown report saved to: {md_path}')\n",
    "\n",
    "# Generate and save HTML report\n",
    "html_path = reporter.save_report('html')\n",
    "print(f'HTML report saved to: {html_path}')\n",
    "\n",
    "print('\\nEvaluation and visualization tutorial completed!')\n",
    "print('You now have tools for:')\n",
    "print('- Model evaluation with multiple metrics')\n",
    "print('- Interactive visualization dashboards')\n",
    "print('- Model comparison frameworks')\n",
    "print('- Advanced evaluation metrics')\n",
    "print('- Comprehensive report generation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}